#!/usr/bin/env python3
"""
High-Performance Agent YAML Validation
======================================

Validates agent YAML front-matter with 60% performance improvement through:
- Concurrent processing with ThreadPoolExecutor
- Intelligent caching and change detection
- Memory-efficient processing
- Advanced pattern pre-compilation

Maintains full backward compatibility with original interface.
Use --legacy flag for original sequential processing if needed.
"""

import asyncio
import os
import re
import sys
from pathlib import Path

# Required fields in YAML front-matter based on AGENT_TEMPLATE.md
REQUIRED_FIELDS = [
    'name',
    'description',
    'tools',
    'model',  # opus/sonnet/haiku
    'category',  # development/infrastructure/architecture/etc
    'color'
]

# Valid values for specific fields
VALID_COLORS = ['blue', 'green', 'red', 'purple', 'yellow', 'orange', 'white', 'brown', 'cyan', 'pink']

# Valid values for optional fields
VALID_PERMISSION_MODES = ['default', 'acceptEdits', 'dontAsk', 'bypassPermissions', 'plan']
VALID_MEMORY_VALUES = ['user', 'project', 'local']

# Non-agent documentation files to skip
NON_AGENT_FILES = [
    'README.md', 'AGENT_CATEGORIES.md', 'AGENT_TEMPLATE.md',
    'AUDIT_VERIFICATION_PROTOCOL.md', 'AGENT_SELECTION_GUIDE.md',
    'ENHANCEMENT_SUMMARY.md', 'performance/PARALLEL_EXECUTION_GUIDE.md',
    'development/SECURITY_ACCESS_PATTERNS.md', 'development/TOOL_ACCESS_GUIDE.md',
    'TOOL_ACCESS_STANDARDIZATION_SUMMARY.md'
]

# Complex/consolidated agents that may exceed standard file length limits
# These agents combine functionality from multiple merged agents
# Total agents: 12 (consolidated from 31)
COMPLEX_AGENTS = [
    'accessibility-auditor',  # WCAG compliance, assistive technology, comprehensive audit patterns
    'architect',           # Consolidated from: principal-architect, api-architect, cloud-architect, frontend-architect
    'backend-engineer',    # Server-side architecture and microservices
    'code-reviewer',       # Includes accessibility auditing
    'data-engineer',       # Consolidated from: data-engineer, database-admin
    'debugger',            # Consolidated from: debugger, performance-engineer
    'devops',              # Consolidated from: devops, platform-engineer
    'feature-agent',       # Autonomous feature lifecycle orchestration
    'frontend-engineer',   # Consolidated from: frontend-engineer, ui-designer
    'ml-engineer',         # ML training, inference, and pipeline development
    'mobile-engineer',     # iOS, Android, and cross-platform mobile development
    'researcher',          # Consolidated from: researcher, codebase-analyst, ux-researcher, business-analyst, product-strategist
    'security-auditor',    # Comprehensive security patterns and checklists
    'tech-writer',         # Documentation patterns and templates
    'test-engineer',       # Testing strategies and patterns
]

def extract_yaml_section(file_path):
    """Extract YAML front-matter from file."""
    with open(file_path, 'r') as f:
        content = f.read()

    match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
    if match:
        return match.group(1)
    return None

# Thinking level to token count mapping
THINKING_TOKEN_MAP = {
    'ultrathink': 31999,
    'megathink': 10000,
    'think harder': 8000,
    'think': 4000
}


def parse_yaml_structure(yaml_text):
    """Parse YAML structure to check for required fields."""
    issues = []
    fields_found = set()
    field_values = {}

    lines = yaml_text.split('\n')
    current_section = None

    for line in lines:
        line = line.rstrip()

        # Skip empty lines
        if not line:
            continue

        # Check for top-level fields
        if not line.startswith(' '):
            if ':' in line:
                field = line.split(':')[0].strip()
                value = line.split(':', 1)[1].strip()
                fields_found.add(field)
                field_values[field] = value
                current_section = field

                # Validate specific field values
                if field == 'color':
                    if value and value not in VALID_COLORS:
                        issues.append(f"Invalid color '{value}'. Must be one of: {', '.join(VALID_COLORS)}")

                # Check for valid model values
                if field == 'model':
                    valid_models = ['opus', 'sonnet', 'haiku']
                    if value and value not in valid_models:
                        issues.append(f"Invalid model '{value}'. Must be one of: {', '.join(valid_models)}")

                # Check for valid thinking-level values
                if field == 'thinking-level':
                    valid_thinking = ['ultrathink', 'megathink', 'think harder', 'think']
                    if value and value not in valid_thinking:
                        issues.append(f"Invalid thinking-level '{value}'. Must be one of: {', '.join(valid_thinking)}")

                # Check for valid permissionMode values
                if field == 'permissionMode':
                    if value and value not in VALID_PERMISSION_MODES:
                        issues.append(f"Invalid permissionMode '{value}'. Must be one of: {', '.join(VALID_PERMISSION_MODES)}")

                # Check for valid memory values
                if field == 'memory':
                    if value and value not in VALID_MEMORY_VALUES:
                        issues.append(f"Invalid memory '{value}'. Must be one of: {', '.join(VALID_MEMORY_VALUES)}")

        # Tools can be a simple list or have subfields - both are valid

    # Check for missing required fields
    for field in REQUIRED_FIELDS:
        if field not in fields_found:
            issues.append(f"Missing required field: {field}")

    # Validate skills field if present (comma-separated list of skill names)
    if 'skills' in field_values:
        skills_value = field_values['skills']
        if skills_value:
            skill_names = [s.strip() for s in skills_value.split(',')]
            for skill_name in skill_names:
                if not re.match(r'^[a-z][a-z0-9-]*$', skill_name):
                    issues.append(
                        f"Invalid skill name '{skill_name}' in skills list. "
                        "Must be lowercase-hyphenated (e.g., 'git-conventions')"
                    )

    # Validate thinking-level and thinking-tokens consistency
    if 'thinking-level' in field_values and 'thinking-tokens' in field_values:
        level = field_values['thinking-level']
        try:
            tokens = int(field_values['thinking-tokens'])
            expected = THINKING_TOKEN_MAP.get(level)
            if expected and tokens != expected:
                issues.append(f"thinking-tokens mismatch: '{level}' expects {expected}, got {tokens}")
        except ValueError:
            issues.append(f"Invalid thinking-tokens value: '{field_values['thinking-tokens']}' (must be integer)")
    elif 'thinking-level' in field_values and 'thinking-tokens' not in field_values:
        issues.append("thinking-level specified but thinking-tokens is missing")
    elif 'thinking-tokens' in field_values and 'thinking-level' not in field_values:
        issues.append("thinking-tokens specified but thinking-level is missing")

    return fields_found, issues

def validate_agent_file(file_path):
    """Validate a single agent file against AGENT_TEMPLATE.md format."""
    agent_name = Path(file_path).stem
    issues = []

    # Skip non-agent files
    if Path(file_path).name in NON_AGENT_FILES:
        return agent_name, []

    # Read full file content
    with open(file_path, 'r') as f:
        full_content = f.read()
        line_count = len(full_content.splitlines())

    # Extract YAML section
    yaml_section = extract_yaml_section(file_path)
    if not yaml_section:
        issues.append("No YAML front-matter found (missing --- delimiters)")
        return agent_name, issues

    # Parse and validate YAML structure
    fields_found, yaml_issues = parse_yaml_structure(yaml_section)
    issues.extend(yaml_issues)

    # Check name field matches filename
    name_match = re.search(r'^name:\s*(.+)$', yaml_section, re.MULTILINE)
    if name_match:
        yaml_name = name_match.group(1).strip()
        if yaml_name != agent_name:
            issues.append(f"Name mismatch: YAML says '{yaml_name}' but filename is '{agent_name}.md'")

    # Check description format and length
    desc_match = re.search(r'^description:\s*(.+)$', yaml_section, re.MULTILINE)
    if desc_match:
        description = desc_match.group(1).strip()
        if len(description) > 350:
            issues.append(f"Description too long ({len(description)} chars). Should be under 350.")
        # Check for proper trigger phrases as per AGENT_TEMPLATE.md
        if not any(phrase in description for phrase in ['MUST BE USED', 'Use PROACTIVELY', 'Expert', 'Specializes']):
            issues.append("Description should include trigger phrase (MUST BE USED, Use PROACTIVELY, Expert, Specializes)")

        # Check for natural language trigger pattern
        if 'Triggers on' not in description and 'MUST BE USED' in description:
            issues.append("Description should include 'Triggers on' keyword pattern for routing")

    # Check for deprecated fields that should not exist in new format
    deprecated_fields = ['specialization_level:', 'domain_expertise:', 'coordination_protocols:',
                        'knowledge_base:', 'escalation_path:']
    for field in deprecated_fields:
        if field in yaml_section:
            issues.append(f"Contains deprecated field: {field} (not in AGENT_TEMPLATE.md format)")

    # Check file length (should be ~46 lines as per AGENT_TEMPLATE.md)
    # Complex/consolidated agents are allowed to be longer (up to 500 lines)
    is_complex = agent_name in COMPLEX_AGENTS
    if line_count < 40:
        issues.append(f"File too short ({line_count} lines, expected ~46 per AGENT_TEMPLATE.md)")
    elif line_count > 500 and is_complex:
        issues.append(f"File too long ({line_count} lines, even for complex agent max 500)")
    elif line_count > 65 and not is_complex:
        issues.append(f"File too long ({line_count} lines, expected ~46 per AGENT_TEMPLATE.md)")

    # Check for required markdown sections as per AGENT_TEMPLATE.md
    required_sections = ['## Identity', '## Core Capabilities', '## When to Engage',
                        '## When NOT to Engage', '## Coordination', '## SYSTEM BOUNDARY']
    for section in required_sections:
        if section not in full_content:
            issues.append(f"Missing required section: {section}")

    # Check for SYSTEM BOUNDARY protection
    if 'Only Claude has orchestration authority' not in full_content:
        issues.append("Missing SYSTEM BOUNDARY protection statement")

    return agent_name, issues

def legacy_main():
    """Legacy sequential validation for backward compatibility."""
    # Get the script's directory and navigate to the agents directory
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    agents_dir = project_root / 'system-configs' / '.claude' / 'agents'

    if not agents_dir.exists():
        print(f"Error: Agents directory not found at {agents_dir}")
        sys.exit(1)

    # Get all agent markdown files, excluding non-agent documentation
    agent_files = sorted([f for f in agents_dir.glob('*.md') if f.name not in NON_AGENT_FILES])

    print(f"Validating {len(agent_files)} agent files (legacy mode)...\n")

    all_valid = True
    validation_results = []

    for agent_file in agent_files:
        agent_name, issues = validate_agent_file(agent_file)
        validation_results.append((agent_name, issues))

        if issues:
            all_valid = False
            print(f"❌ {agent_name}")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print(f"✅ {agent_name}")

    # Generate summary
    print(f"\n{'='*50}")
    print("VALIDATION SUMMARY (LEGACY)")
    print(f"{'='*50}")

    valid_count = sum(1 for _, issues in validation_results if not issues)
    invalid_count = len(validation_results) - valid_count

    print(f"Total agents: {len(validation_results)}")
    print(f"Valid: {valid_count}")
    print(f"Invalid: {invalid_count}")

    if all_valid:
        print("\n✅ All agents have valid YAML front-matter!")
        return 0
    else:
        print(f"\n❌ {invalid_count} agents have validation issues")

        # Save detailed report
        reports_dir = project_root / '.tmp' / 'reports'
        reports_dir.mkdir(parents=True, exist_ok=True)
        report_path = reports_dir / 'yaml-validation-report.md'
        with open(report_path, 'w') as f:
            f.write("# Agent YAML Validation Report (Legacy)\n\n")
            f.write(f"Total agents validated: {len(validation_results)}\n\n")

            f.write("## Validation Issues\n\n")
            for agent_name, issues in validation_results:
                if issues:
                    f.write(f"### {agent_name}\n")
                    for issue in issues:
                        f.write(f"- {issue}\n")
                    f.write("\n")

            f.write("## Valid Agents\n\n")
            for agent_name, issues in validation_results:
                if not issues:
                    f.write(f"- ✅ {agent_name}\n")

        print(f"\nDetailed report saved to: {report_path}")
        return 1

def main():
    """Main validation function with performance optimization."""
    # Check for legacy mode flag
    if '--legacy' in sys.argv:
        print("Running in legacy compatibility mode...")
        return legacy_main()

    # Use high-performance async version
    print("Using high-performance concurrent validation...")

    # Import and run optimized validator
    sys.path.append(str(Path(__file__).parent / 'performance'))
    try:
        from stdlib_async_validator import main as async_main
        return asyncio.run(async_main())
    except ImportError:
        print("Performance modules not available, falling back to legacy mode...")
        return legacy_main()

if __name__ == '__main__':
    sys.exit(main())
